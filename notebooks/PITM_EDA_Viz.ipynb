{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:16:50.195590Z",
     "start_time": "2019-11-05T19:16:48.331795Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pdb\n",
    "from pymongo import MongoClient\n",
    "from pymongo import InsertOne, DeleteOne, ReplaceOne, UpdateMany, UpdateOne\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:16:52.429071Z",
     "start_time": "2019-11-05T19:16:52.424763Z"
    }
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.polymedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:17:27.987661Z",
     "start_time": "2019-11-05T19:17:27.983128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subreddit_polyamory', 'temp', 'pitm', 'test']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:17:40.208921Z",
     "start_time": "2019-11-05T19:17:40.205693Z"
    }
   },
   "outputs": [],
   "source": [
    "pitm = db.pitm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:17:44.874842Z",
     "start_time": "2019-11-05T19:17:44.870755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_id', 'raw_post_html', 'raw_comments_html', 'num_comments', 'post_date_string', 'post_date', 'post_title', 'quotes', 'editorial_text', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "pprint(pitm.find_one().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:46:11.794050Z",
     "start_time": "2019-11-05T19:46:11.789690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "')    Cartório de São Paulo registra união estável de três pessoas.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "escape_ansi(')  -------------  Cartório de São Paulo registra união estável de três pessoas.').replace('-','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:50:29.278371Z",
     "start_time": "2019-11-05T19:50:29.261878Z"
    },
    "code_folding": [
     6,
     11,
     26,
     38,
     43,
     48
    ]
   },
   "outputs": [],
   "source": [
    "def escape_ansi(line):\n",
    "    ansi_escape = re.compile(r'(\\x9B|\\x1B\\[)[0-?]*[ -\\/]*[@-~]+')\n",
    "    out = ansi_escape.sub('', line)\n",
    "    out = re.sub('[%s]' % re.escape(string.punctuation), ' ', out.replace('--',''))\n",
    "    return out\n",
    "\n",
    "\n",
    "def make_soup(url):\n",
    "    webpage_response = requests.get(url)\n",
    "    return BeautifulSoup(webpage_response.content, 'html.parser')\n",
    "\n",
    "\n",
    "def get_all_a_hrefs(soup, selector=''):\n",
    "    \"\"\"\n",
    "    Grabs all href attribute values from a tags\n",
    "    contained inside given selector.\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    if len(selector) > 0:\n",
    "        prefix = selector + ' '\n",
    "    else:\n",
    "        prefix = ''\n",
    "    for a in soup.select(prefix + 'a', href=True):\n",
    "        links += [a['href']]\n",
    "    return list(set(links))\n",
    "\n",
    "\n",
    "def is_link_date_archive(link):\n",
    "    \"\"\"\n",
    "    Identifies links with potential date value in url path.\n",
    "    \"\"\"\n",
    "    # print(link)\n",
    "    # print(re.search('.+\\/[0-9]{4}\\/[0-9]{2}\\/', link))\n",
    "    if re.search('.+[0-9]{4}/[0-9]{2}.+', link) is None:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def list_outbound_links(page_url, domain):\n",
    "    soup = make_soup(page_url)\n",
    "    links = get_all_a_hrefs(soup)\n",
    "    return [link for link in links if not((re.search('.+domain.+', link)))]\n",
    "\n",
    "def get_post_permalinks(archive_url):\n",
    "    soup = make_soup(archive_url)\n",
    "    permalinks = soup.select('a[title=\"permanent link\"]')\n",
    "    return [p['href'] for p in permalinks]\n",
    "\n",
    "def scrape_posts(permalinks):\n",
    "    \n",
    "    cols = []\n",
    "\n",
    "    # Iterate through posts and create dictionary:\n",
    "    for p in permalinks:\n",
    "        \n",
    "        soup = make_soup(p)\n",
    "        date_header = soup.select('.date-header')[0]\n",
    "        post = soup.select('.post')[0]\n",
    "        \n",
    "        try:\n",
    "            comments = soup.select('#comments')[0]\n",
    "            num_comments = int(comments.select('h4')[0].get_text().split()[0])\n",
    "            comment_blocks = [c.text.strip()\n",
    "                      for c in post.find_all(\".comment-body\")]\n",
    "        except:\n",
    "            num_comments = 0\n",
    "            comment_blocks = []\n",
    "\n",
    "        post_col = {}\n",
    "\n",
    "        # Save raw HTML for later, just in case:\n",
    "        post_col['raw_post_html'] = post\n",
    "        post_col['raw_comments_html'] = comments\n",
    "        \n",
    "        # Save comments:\n",
    "        post_col['comments'] = comment_blocks\n",
    "        \n",
    "        # Post meta:\n",
    "        post_col['num_comments'] = num_comments\n",
    "        post_col['post_date_string'] = date_header.get_text()\n",
    "        post_col['post_date'] = parse(date_header.get_text())\n",
    "        post_col['post_title'] = post.select('h3.post-title')[0].get_text().strip()\n",
    "\n",
    "        # Collect all blockquotes from news sources:\n",
    "        quotes = post.find_all(\"blockquote\")\n",
    "        quote_texts = [q.get_text().strip()\n",
    "                  for q in quotes]\n",
    "        post_col['quotes'] = quote_texts\n",
    "        \n",
    "\n",
    "        \n",
    "        # Remove quotes from the main HTML, leaving blog author's commentary:\n",
    "        for q in quotes:\n",
    "            q.extract()\n",
    "        \n",
    "        post_col['editorial_text'] = post.text.strip()\n",
    "\n",
    "        cols += [post_col]\n",
    "\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-proccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:41:44.868656Z",
     "start_time": "2019-11-05T19:41:44.857032Z"
    }
   },
   "outputs": [],
   "source": [
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace('\\t',' <stop>')\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [escape_ansi(s) for s in sentences if len(s)>1]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:18:03.255097Z",
     "start_time": "2019-11-05T19:18:03.246186Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def clean_text(texts, lancaster=False):\n",
    "    \n",
    "    def stem_words(seq):\n",
    "        \n",
    "#         print('seq to stem:')\n",
    "#         print(seq)\n",
    "        if lancaster:\n",
    "            st = LancasterStemmer()\n",
    "            \n",
    "            stemmed = \" \".join([st.stem(w) for w in seq.split()])\n",
    "#             print(stemmed)\n",
    "            return stemmed\n",
    "    \n",
    "    def process(text):\n",
    "        \n",
    "        rep = {\"\\'\": \"\",\n",
    "               \"\\xa0\": \" \",\n",
    "               \"  \": ' ',\n",
    "               \"\\n\":\".\",\n",
    "               \"\\t\":\".\",\n",
    "               \"\\x97\": \" \"\n",
    "              }\n",
    "        rep = OrderedDict((re.escape(k), v) for k, v in rep.items()) \n",
    "        pattern = re.compile(\"|\".join(rep.keys()))\n",
    "        text = pattern.sub(lambda m: rep[re.escape(m.group(0))], text)\n",
    "        \n",
    "        clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "        clean_text = clean_text.lower()\n",
    "        clean_text = re.sub('\\w*\\d\\w*', ' ', clean_text)\n",
    "#         clean_text = re.sub('[\\n|\\t]', ' ', clean_text)\n",
    "\n",
    "        if lancaster:\n",
    "            return stem_words(clean_text)\n",
    "        else:\n",
    "            return clean_text\n",
    "\n",
    "    if type(texts) == list:\n",
    "        return [process(t) for t in texts]\n",
    "\n",
    "    else:\n",
    "        return process(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-process PITM Quote / Editorial Text\n",
    "\n",
    "I should have split by sentence level..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:23:37.012425Z",
     "start_time": "2019-11-05T19:23:37.009754Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T20:04:09.500074Z",
     "start_time": "2019-11-05T20:04:09.495324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_id', 'raw_post_html', 'raw_comments_html', 'num_comments', 'post_date_string', 'post_date', 'post_title', 'quotes', 'editorial_text', 'labels', 'post_sentences', 'all_post_sentences', 'quotes_by_sentence', 'editorial_sentences'])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitm.find_one({}).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:05:49.673960Z",
     "start_time": "2019-11-05T18:05:40.082488Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T20:09:23.926137Z",
     "start_time": "2019-11-05T20:09:23.835740Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5dbf0bda858e4cbd747f5fb1'),\n",
       " 'raw_post_html': '<div class=\"post\"><a name=\"595843825317736586\"></a>\\n<h3 class=\"post-title\">\\n                      \\t \\n                      \\t \"A polyamorous quad welcomes their first child\"\\n\\t                       \\n                          </h3>\\n<div class=\"post-body\">\\n<p><div style=\"clear:both;\"></div><span style=\"font-size: 100%; font-weight: bold; font-style: italic; font-size:116%;\">Offbeat Mama<br/></span><br><a href=\"http://media.offbeatmama.com/wp-content/blogs.dir/2/files/2012/09/Connor-and-Family1-500x386.jpg\" style=\"font-size: 100%; \"><img alt=\"\" border=\"0\" src=\"https://lh3.googleusercontent.com/proxy/Ox0RixY-IYB2mBiGOtrIjkUaPUzKwv_X563_bnDIwfxiY7QsobxYx3_b_YN-LqVgqgcKqvE0r5UgXpNrfcvUguOLMQttBDsG5WxCcdyjZkjvsphGRJo3qAqjYzzqUkwWFsS6L264jRxExRm4hUDqpKCEXAQ=s0-d\" style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;width: 500px; height: 386px;\"/></a>\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<i><span style=\"font-size:85%;\">Connor and family. Left to right: Ian, Micah (the author), Aimee, Michelle.</span></i><br><br><span style=\"font-size: 100%;\">For its \"birth week,\" the online magazine Offbeat Mama (\"Parenting against the grain\") presents an article by a friend of mine who co-organizes the </span><a href=\"http://www.transcendingboundaries.org/\" style=\"font-size: 100%; \" target=\"new_window\">Transcending Boundaries conference</a><span style=\"font-size: 100%;\">, held in Massachusetts every fall to bridge across queer, alt-gender, and alt-relationship identities.</span><br><br><br><a href=\"http://offbeatmama.com/2012/09/polyamorous-quad-has-first-baby\" style=\"font-size: 100%; \" target=\"new_window\">Read on!</a><span style=\"font-size: 100%;\"> (Sept. 10, 2012).</span><br><br><span style=\"font-size: 100%;\">That was 17 months ago. Last I saw Connor he was thriving, vigorous and adorable.</span><br><br><span style=\"font-size: 100%;\">Micah says they\\'re planning more.</span><br><br><span style=\"font-size: 100%;\">He also writes,</span><br><br><br><a href=\"http://polyinthemedia.blogspot.com/2012/09/a-polyamorous-quad-welcomes-their-first.html\" style=\"font-size: 100%; \" target=\"new_window\">[Permalink]</a><div style=\"clear:both; padding-bottom:0.25em\"></div><p class=\"blogger-labels\">Labels: <a href=\"http://polyinthemedia.blogspot.com/search/label/poly%20parenting\" rel=\"tag\">poly parenting</a>, <a href=\"http://polyinthemedia.blogspot.com/search/label/quads\" rel=\"tag\">quads</a>, <a href=\"http://polyinthemedia.blogspot.com/search/label/Show%20Your%20Parents\" rel=\"tag\">Show Your Parents</a>, <a href=\"http://polyinthemedia.blogspot.com/search/label/The%20Best\" rel=\"tag\">The Best</a></p></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></p>\\n</div>\\n<p class=\"post-footer\">posted by Alan | <a href=\"http://polyinthemedia.blogspot.com/2012/09/a-polyamorous-quad-welcomes-their-first.html\" title=\"permanent link\"></a>\\n<span class=\"item-action\"><a href=\"https://www.blogger.com/email-post.g?blogID=30436440&amp;postID=595843825317736586\" title=\"Email Post\"><img alt=\"\" class=\"icon-action\" height=\"13\" src=\"https://resources.blogblog.com/img/icon18_email.gif\" width=\"18\"/></a></span><span class=\"item-control blog-admin pid-206420556\"><a href=\"https://www.blogger.com/post-edit.g?blogID=30436440&amp;postID=595843825317736586&amp;from=pencil\" style=\"border:none;\" title=\"Edit Post\"><img alt=\"\" class=\"icon-action\" height=\"18\" src=\"https://resources.blogblog.com/img/icon18_edit_allbkg.gif\" width=\"18\"/></a></span> </p>\\n</div>',\n",
       " 'quotes': ['A polyamorous quad welcomes their first childThis is not a \"normal\" birth story. Which makes sense, since my family is not a normal family. Please note the lack of quotes that second time — it\\'s with good reason....On April 2, 2011 our little Munchkin came into this world, caught in the loving hands of two of his parents while a third held his mother. We were surrounded by a top-quality, professional staff made up of midwives, nurses, and our doula. His entry into this world went exactly as we wanted, with minimal intervention, surrounded by love and full of hope for the future....Aimee\\'s due date was Friday, March 30. We\\'d all been anxiously waiting for the big day seemingly forever. For most of the previous three weeks, every little exclamation from Aimee was met with the same question, \"Are you OK?\" from one or all of her partners. How she didn\\'t kill any of us is testimony to her prodigious patience.Friday began like any other day in our household.... At the midwife appointment Aimee was [dilated] between one and two centimeters, so all of the practice her uterus had been up to was doing something. The nurse cautioned that things could stay like this for a few more days, or she could suddenly open up and give birth in a few hours. With that in mind we went home. Around 8:25 pm, Aimee gave out an exclamation that was not as little as before. Then another. She was painfully cramping along with the Braxton-Hicks contractions. This was something new....Like a good geek girl, Aimee got a little app for her iPhone that created a log of her contractions. Push a button to start the timer, push it again to stop, and it created a handy dandy log of your use. She fired it up and started using it to get good data....If this was early labor, we knew it could be a long process, or even stop. Ian and Mich went to bed, determined to rest in case it was the last good night\\'s sleep they\\'d have in awhile. Aimee was too uncomfortable to really sleep, so she and I bunked down on our sectional, watched movies, and settled in for a long night.Friday night was a very long night....At this point (around 7am Saturday morning), Aimee had been awake for almost twenty-four hours, and in labor for eleven of them. I point this out because my admiration for her just went up, which is saying something, as the rest of the story will show....By the time we made it to our [hospital] room, we were all hungry, since breakfast had been four hours before and none of us had eaten much. Excitement does that to the appetite. I had just tweeted to the world that we were safely ensconced.......Here is where Barbara really demonstrated why every pregnant woman should have a doula. She gave expert advice on positioning, encouraging Aimee to shift positions frequently to prevent fatigue. She had, of all things, a piece of shelf liner (the rubbery mesh kind that keeps things from sliding around), that she used to hold up Aimee\\'s belly. Taking the weight off of her, for even a few minutes at a time, was an invaluable relief. And I cannot underestimate how important her presence was for Ian, Michy and I....',\n",
       "  \"I've been very pleased with the reception on Offbeat Mama. I've been encouraged to write more about our family for them, and I am planning to do so.Now that Connor is 17 months, the one thing I can say is that he is a pretty normal toddler. He has his own little personality quirks, like all kids do. He loves motorized vehicles, the bigger and louder the better. He's starting to show strong preferences regarding food and clothing. And he absolutely loves sports. And Yo GabaGaba. But overall, he's growing and developing exactly as he should be. Which would come as no surprise to anyone reading your blog. Poly families aren't that different from mono ones. We just have a couple extra sets of hands.\"],\n",
       " 'all_post_sentences': [' A polyamorous quad welcomes their first child ',\n",
       "  'Offbeat Mama  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Connor and family ',\n",
       "  'Left to right  Ian  Micah  the author   Aimee  Michelle ',\n",
       "  'For its  birth week   the online magazine Offbeat Mama   Parenting against the grain   presents an article by a friend of mine who co organizes the Transcending Boundaries conference  held in Massachusetts every fall to bridge across queer  alt gender  and alt relationship identities ',\n",
       "  'Read on ',\n",
       "  ' Sept ',\n",
       "  '10  2012  ',\n",
       "  'That was 17 months ago ',\n",
       "  'Last I saw Connor he was thriving  vigorous and adorable ',\n",
       "  'Micah says they re planning more '],\n",
       " 'quotes_by_sentence': [],\n",
       " 'editorial_sentences': [' A polyamorous quad welcomes their first child ',\n",
       "  'Offbeat Mama  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Connor and family ',\n",
       "  'Left to right  Ian  Micah  the author   Aimee  Michelle ',\n",
       "  'For its  birth week   the online magazine Offbeat Mama   Parenting against the grain   presents an article by a friend of mine who co organizes the Transcending Boundaries conference  held in Massachusetts every fall to bridge across queer  alt gender  and alt relationship identities ',\n",
       "  'Read on ',\n",
       "  ' Sept ',\n",
       "  '10  2012  ',\n",
       "  'That was 17 months ago ',\n",
       "  'Last I saw Connor he was thriving  vigorous and adorable ',\n",
       "  'Micah says they re planning more ']}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = list(pitm.find({},{'raw_post_html':1, 'all_post_sentences':1, 'quotes_by_sentence':1, 'editorial_sentences':1, 'quotes':1}))\n",
    "posts[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T20:05:37.773940Z",
     "start_time": "2019-11-05T20:05:37.732737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for post in posts[:5]:\n",
    "    raw_post_html = post['raw_post_html']\n",
    "    \n",
    "    soup = BeautifulSoup(raw_post_html)\n",
    "    \n",
    "    quotes = soup.find_all(\"blockquote\")\n",
    "    \n",
    "    for s in soup.findAll('br'):\n",
    "        s.replaceWith(\" \")\n",
    "    \n",
    "    # Sentences as documents for each post\n",
    "    sentences = split_into_sentences(soup.text)\n",
    "    pitm.update_one({'_id': post['_id']}, {\"$set\": { \"all_post_sentences\": sentences} })\n",
    "    \n",
    "    # get just the quotes\n",
    "    quotes = soup.find_all(\"blockquote\")\n",
    "    print(quotes)\n",
    "    quotes_list = []\n",
    "    for i, quote in enumerate(quotes):\n",
    "        quotes_dict = {}\n",
    "        sentences = split_into_sentences(quote.text)\n",
    "        quotes_dict['sentences'] = sentences\n",
    "        print(quotes_dict)\n",
    "        quotes_list += [quotes_dict]\n",
    "        \n",
    "    pitm.update_one({'_id': post['_id']}, {\"$set\": { \"quotes_by_sentence\": quotes_list} })\n",
    "        \n",
    "    for q in quotes:\n",
    "        q.extract()\n",
    "    \n",
    "    sentences = split_into_sentences(soup.text)\n",
    "    pitm.update_one({'_id': post['_id']}, {\"$set\": { \"editorial_sentences\": sentences} })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:29:15.393514Z",
     "start_time": "2019-11-05T19:29:15.385659Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New Jersey newspaper columnist  comes around',\n",
       " \"The Trentonian  A month ago I mentioned a New Jersey newspaper columnist's dismissive freakout about the concept behind Showtime's Polyamory: Married and Dating.\",\n",
       " \"Apparently some of you wrote him well-considered letters, enough that he's now written a second, much more conciliatory column about your responses.\",\n",
       " 'What struck him in particular was how closeted poly people feel they need to be, unlike people who just date around.',\n",
       " 'See the original article (Aug.',\n",
       " '28, 2012).']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_post_html = pitm.find_one({},{'_id':0, 'raw_post_html':1})['raw_post_html']\n",
    "soup = BeautifulSoup(raw_post_html)\n",
    "for s in soup.findAll('br'):\n",
    "    s.replaceWith(\" \")\n",
    "split_into_sentences(soup.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:27:01.498413Z",
     "start_time": "2019-11-05T19:27:01.490399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><div class=\"post\"><a name=\"3690396103602027663\"></a>\n",
       "<h3 class=\"post-title\">\n",
       "                      \t \n",
       "                      \t New Jersey newspaper columnist <br/>comes around\n",
       "\t                       \n",
       "                          </h3>\n",
       "<div class=\"post-body\">\n",
       "<p></p><div style=\"clear:both;\"></div><span style=\"FONT-WEIGHT: bold; FONT-STYLE: italic; FONT-SIZE: 116%\">The Trentonian<br/></span><br/>A month ago I mentioned a New Jersey newspaper columnist's <a href=\"http://www.trentonian.com/article/20120807/OPINION03/120809778/sex-with-your-spouse-his-spouse-him-your-neighbor--&amp;pager=2\" target=\"new_window\">dismissive freakout</a> about the concept behind Showtime's <i>Polyamory: Married and Dating.</i> Apparently some of you wrote him well-considered letters, enough that he's now written a second, much more conciliatory column about your responses. What struck him in particular was how closeted poly people feel they need to be, unlike people who just date around.<br/><br/><br/>See the <a href=\"http://www.trentonian.com/article/20120828/OPINION03/120829994/multiple-lovers-stuck-in-the-closet\" target=\"new_window\">original article</a> (Aug. 28, 2012).<br/><br/><a href=\"\" target=\"new_window\">[Permalink]</a><div style=\"clear:both; padding-bottom:0.25em\"></div>\n",
       "</div>\n",
       "<p class=\"post-footer\">posted by Alan | <a href=\"http://polyinthemedia.blogspot.com/2012/08/new-jersey-newspaper-columnist-comes.html\" title=\"permanent link\"></a>\n",
       "<span class=\"item-action\"><a href=\"https://www.blogger.com/email-post.g?blogID=30436440&amp;postID=3690396103602027663\" title=\"Email Post\"><img alt=\"\" class=\"icon-action\" height=\"13\" src=\"https://resources.blogblog.com/img/icon18_email.gif\" width=\"18\"/></a></span><span class=\"item-control blog-admin pid-206420556\"><a href=\"https://www.blogger.com/post-edit.g?blogID=30436440&amp;postID=3690396103602027663&amp;from=pencil\" style=\"border:none;\" title=\"Edit Post\"><img alt=\"\" class=\"icon-action\" height=\"18\" src=\"https://resources.blogblog.com/img/icon18_edit_allbkg.gif\" width=\"18\"/></a></span> </p>\n",
       "</div></body></html>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:27:01.916617Z",
     "start_time": "2019-11-05T19:27:01.909339Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><div class=\"post\"><a name=\"3690396103602027663\"></a>\n",
       "<h3 class=\"post-title\">\n",
       "                      \t \n",
       "                      \t New Jersey newspaper columnist  . comes around\n",
       "\t                       \n",
       "                          </h3>\n",
       "<div class=\"post-body\">\n",
       "<p></p><div style=\"clear:both;\"></div><span style=\"FONT-WEIGHT: bold; FONT-STYLE: italic; FONT-SIZE: 116%\">The Trentonian . </span> . A month ago I mentioned a New Jersey newspaper columnist's <a href=\"http://www.trentonian.com/article/20120807/OPINION03/120809778/sex-with-your-spouse-his-spouse-him-your-neighbor--&amp;pager=2\" target=\"new_window\">dismissive freakout</a> about the concept behind Showtime's <i>Polyamory: Married and Dating.</i> Apparently some of you wrote him well-considered letters, enough that he's now written a second, much more conciliatory column about your responses. What struck him in particular was how closeted poly people feel they need to be, unlike people who just date around. .  .  . See the <a href=\"http://www.trentonian.com/article/20120828/OPINION03/120829994/multiple-lovers-stuck-in-the-closet\" target=\"new_window\">original article</a> (Aug. 28, 2012). .  . <a href=\"\" target=\"new_window\">[Permalink]</a><div style=\"clear:both; padding-bottom:0.25em\"></div>\n",
       "</div>\n",
       "<p class=\"post-footer\">posted by Alan | <a href=\"http://polyinthemedia.blogspot.com/2012/08/new-jersey-newspaper-columnist-comes.html\" title=\"permanent link\"></a>\n",
       "<span class=\"item-action\"><a href=\"https://www.blogger.com/email-post.g?blogID=30436440&amp;postID=3690396103602027663\" title=\"Email Post\"><img alt=\"\" class=\"icon-action\" height=\"13\" src=\"https://resources.blogblog.com/img/icon18_email.gif\" width=\"18\"/></a></span><span class=\"item-control blog-admin pid-206420556\"><a href=\"https://www.blogger.com/post-edit.g?blogID=30436440&amp;postID=3690396103602027663&amp;from=pencil\" style=\"border:none;\" title=\"Edit Post\"><img alt=\"\" class=\"icon-action\" height=\"18\" src=\"https://resources.blogblog.com/img/icon18_edit_allbkg.gif\" width=\"18\"/></a></span> </p>\n",
       "</div></body></html>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for s in soup.findAll('br'):\n",
    "    s.replaceWith(\" . \")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:20:17.785944Z",
     "start_time": "2019-11-05T19:20:17.779183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New Jersey newspaper columnist comes around',\n",
       " \"The TrentonianA month ago I mentioned a New Jersey newspaper columnist's dismissive freakout about the concept behind Showtime's Polyamory: Married and Dating.\",\n",
       " \"Apparently some of you wrote him well-considered letters, enough that he's now written a second, much more conciliatory column about your responses.\",\n",
       " 'What struck him in particular was how closeted poly people feel they need to be, unlike people who just date around.',\n",
       " 'See the original article (Aug.',\n",
       " '28, 2012).']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_into_sentences(soup.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Site - polyinthemedia.blogspot.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T15:43:59.839860Z",
     "start_time": "2019-11-05T15:43:59.191751Z"
    }
   },
   "outputs": [],
   "source": [
    "# Study the editorial writings of the blogger who has compiled this list of articles.\n",
    "\n",
    "editorial_corpus = list(posts.find({},{'_id':0, 'editorial_text':1, 'post_date':1}))\n",
    "# print(editorial_texts[0:5])\n",
    "editorial_dates = [p['post_date'] for p in editorial_corpus]\n",
    "editorial_corpus = [p['editorial_text'] for p in editorial_corpus]\n",
    "editorial_corpus = clean_text(editorial_corpus)\n",
    "len(editorial_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T16:15:10.614942Z",
     "start_time": "2019-11-05T16:15:09.376143Z"
    }
   },
   "outputs": [],
   "source": [
    "# Collect all quoted content from news articles\n",
    "\n",
    "pipeline = [\n",
    "    {'$unwind':'$quotes'},\n",
    "    {'$project': {'_id':0, 'quotes':1, 'post_date':1}}\n",
    "           ]\n",
    "\n",
    "quotes_corpus = list(posts.aggregate(pipeline))\n",
    "\n",
    "quotes_dates = [p['post_date'] for p in quotes_corpus]\n",
    "quotes_corpus = [p['quotes'] for p in quotes_corpus]\n",
    "quotes_corpus = clean_text(quotes_corpus)\n",
    "len(quotes_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T16:58:23.733613Z",
     "start_time": "2019-11-05T16:58:23.729296Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "quotes_corpus[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_corpus(level='sentence'):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(corpus, model=CountVectorizer(stop_words='english', min_df=10)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T20:59:37.467797Z",
     "start_time": "2019-11-03T20:59:37.006837Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count vectorizer on cleaned text\n",
    "cv = CountVectorizer(stop_words='english', min_df=10)\n",
    "X = cv.fit_transform(corpus)\n",
    "_ = pd.DataFrame(X.toarray(), columns=cv.get_feature_names())\n",
    "df = pd.concat([pd.Series(dates, name='post_date'), _], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T20:32:49.826131Z",
     "start_time": "2019-11-03T20:32:49.360796Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.to_csv('../data/exports/editorial_texts_mindf100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancaster, min-df 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T20:50:22.618912Z",
     "start_time": "2019-11-03T20:50:15.295714Z"
    }
   },
   "outputs": [],
   "source": [
    "editorial_texts = list(posts.find({},{'_id':0, 'editorial_text':1, 'post_date':1}))\n",
    "# print(editorial_texts[0:5])\n",
    "dates = [p['post_date'] for p in editorial_texts]\n",
    "corpus = [p['editorial_text'] for p in editorial_texts]\n",
    "corpus = clean_text(corpus, lancaster=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T20:50:27.885090Z",
     "start_time": "2019-11-03T20:50:27.535141Z"
    }
   },
   "outputs": [],
   "source": [
    "# Count vectorizer on cleaned text\n",
    "cv = CountVectorizer(stop_words='english', min_df=40)\n",
    "X = cv.fit_transform(corpus)\n",
    "_ = pd.DataFrame(X.toarray(), columns=cv.get_feature_names())\n",
    "df = pd.concat([pd.Series(dates, name='post_date'), _], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T20:50:30.987107Z",
     "start_time": "2019-11-03T20:50:29.379704Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('../data/exports/editorial_texts_mindf40_lancaster.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngrams (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editorial_texts = list(posts.find({},{'_id':0, 'editorial_text':1, 'post_date':1}))\n",
    "# print(editorial_texts[0:5])\n",
    "dates = [p['post_date'] for p in editorial_texts]\n",
    "corpus = [p['editorial_text'] for p in editorial_texts]\n",
    "corpus = clean_text(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:11:59.699406Z",
     "start_time": "2019-11-03T21:11:55.326015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Count vectorizer on cleaned text\n",
    "cv = CountVectorizer(stop_words='english', min_df=100, ngram_range=(1,5))\n",
    "X = cv.fit_transform(corpus)\n",
    "_ = pd.DataFrame(X.toarray(), columns=cv.get_feature_names())\n",
    "df = pd.concat([pd.Series(dates, name='post_date'), _], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:12:35.851904Z",
     "start_time": "2019-11-03T21:12:35.827925Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectorizer on cleaned text\n",
    "cv = TF(stop_words='english', min_df=100, ngram_range=(1,5))\n",
    "X = cv.fit_transform(corpus)\n",
    "_ = pd.DataFrame(X.toarray(), columns=cv.get_feature_names())\n",
    "df = pd.concat([pd.Series(dates, name='post_date'), _], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T20:58:15.457646Z",
     "start_time": "2019-11-03T20:58:15.454762Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:02:58.529986Z",
     "start_time": "2019-11-03T21:02:58.489350Z"
    }
   },
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(10)\n",
    "doc_topic = lsa.fit_transform(X)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:03:02.387946Z",
     "start_time": "2019-11-03T21:03:02.361288Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "             columns = cv.get_feature_names())\n",
    "topic_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:03:03.528275Z",
     "start_time": "2019-11-03T21:03:03.522787Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:03:04.619355Z",
     "start_time": "2019-11-03T21:03:04.609573Z"
    }
   },
   "outputs": [],
   "source": [
    "display_topics(lsa, cv.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:03:51.067910Z",
     "start_time": "2019-11-03T21:03:51.052180Z"
    }
   },
   "outputs": [],
   "source": [
    "Vt = pd.DataFrame(doc_topic.round(5))\n",
    "Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB Writes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Get post labels from HTML\n",
    "# --------------------------\n",
    "\n",
    "cursor = posts.find({},{'raw_post_html':1,'post_title':1,'labels':1})\n",
    "_ = list(cursor)\n",
    "\n",
    "for s in _:\n",
    "    _id = s['_id']\n",
    "    \n",
    "    # Extract blog post labels\n",
    "    soup = BeautifulSoup(s['raw_post_html'])\n",
    "    labels = [l.text for l in soup.select('.blogger-labels a')]\n",
    "    \n",
    "    # Update MongoDB\n",
    "    requests = [UpdateOne(\n",
    "    {'post_title' : s['post_title']},\n",
    "    { '$set':\n",
    "     {\n",
    "         'labels': labels\n",
    "     }\n",
    "    }\n",
    "    )]\n",
    "    posts.bulk_write(requests)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "224.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
