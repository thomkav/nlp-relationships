{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T21:22:11.029002Z",
     "start_time": "2019-11-07T21:22:08.317278Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "import random\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pdb\n",
    "from pymongo import MongoClient\n",
    "from pymongo import InsertOne, DeleteOne, ReplaceOne, UpdateMany, UpdateOne\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize, wordpunct_tokenize, sent_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk.data\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import OrderedDict\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from gensim.test.utils import common_texts, common_corpus, common_dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T21:22:16.610365Z",
     "start_time": "2019-11-07T21:22:16.607699Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T21:22:16.778979Z",
     "start_time": "2019-11-07T21:22:16.774067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Document Term Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "R/Polyamory, 2012 and 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T21:22:20.590278Z",
     "start_time": "2019-11-07T21:22:20.531998Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "suffix = 'r_poly_features_stemmed_cv'\n",
    "filename = '../data/exports/doc_term/r_polyamory_2012and2019/doc_term_'+suffix+'.pkl'\n",
    "\n",
    "with open(filename, 'rb') as file:\n",
    "    doc_term = pickle.load(file)\n",
    "    \n",
    "with open('../data/exports/doc_term/r_polyamory_2012and2019/date_index.pkl', 'rb') as file:\n",
    "    doc_dates = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T21:22:23.638625Z",
     "start_time": "2019-11-07T21:22:23.308419Z"
    }
   },
   "outputs": [],
   "source": [
    "# All concatenated / flatted threads of r_polyamory from 2012 and 2019\n",
    "with open('../data/exports/by_thread/r_poly_2012_and_2019.pkl', 'rb') as file:\n",
    "    r_poly_threads = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Define Custom Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T21:22:28.865039Z",
     "start_time": "2019-11-07T21:22:28.858927Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "swds_reddit_and_web = ([\n",
    "    'reddit', 'subreddit', 'posts','comments'\n",
    "    'html', 'com','utm', 'www', 'http',\n",
    "    'sub', # not sure if this one should be included...\n",
    "    'medium',\n",
    "    'deleted', 'delete', 'removed',\n",
    "    'x200b',\n",
    "])\n",
    "\n",
    "swds_relationships = ([\n",
    "    \n",
    "])\n",
    "\n",
    "swds_polyamory_terms = ([\n",
    "    'polyamory', 'poly'\n",
    "])\n",
    "\n",
    "swds_artifacts = ([\n",
    "    've','don'\n",
    "])\n",
    "\n",
    "def custom_stop_words(custom_stop_lists, stop_english=True):\n",
    "    '''\n",
    "    \n",
    "    Takes a list of lists of custom stop words,\n",
    "    and forms a custom stop words list.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    swds_custom = []\n",
    "    \n",
    "    if stop_english:\n",
    "        swds_custom.extend(stopwords.words('english'))\n",
    "    \n",
    "    \n",
    "    for wordlist in custom_stop_lists:\n",
    "#         print(wordlist)\n",
    "        if len(wordlist)>0:\n",
    "            swds_custom.extend(wordlist)\n",
    "    \n",
    "    return swds_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T21:23:45.280337Z",
     "start_time": "2019-11-06T21:23:45.275117Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reddit',\n",
       " 'subreddit',\n",
       " 'posts',\n",
       " 'commentshtml',\n",
       " 'com',\n",
       " 'utm',\n",
       " 'www',\n",
       " 'http',\n",
       " 'sub',\n",
       " 'medium',\n",
       " 'deleted',\n",
       " 'delete',\n",
       " 'removed',\n",
       " 'x200b']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_stop_words([swds_reddit_and_web], stop_english=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T21:23:47.031998Z",
     "start_time": "2019-11-06T21:23:47.027540Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reddit',\n",
       " 'subreddit',\n",
       " 'posts',\n",
       " 'commentshtml',\n",
       " 'com',\n",
       " 'utm',\n",
       " 'www',\n",
       " 'http',\n",
       " 'sub',\n",
       " 'medium',\n",
       " 'deleted',\n",
       " 'delete',\n",
       " 'removed',\n",
       " 'x200b',\n",
       " 've',\n",
       " 'don']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_stop_words([swds_reddit_and_web, swds_artifacts], stop_english=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/Select Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T21:22:33.907407Z",
     "start_time": "2019-11-07T21:22:33.883587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'text_concat'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(r_poly_threads)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T20:53:42.776551Z",
     "start_time": "2019-11-06T20:53:42.761340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text_concat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-05 04:13:35</td>\n",
       "      <td>I’ve seen a couple of posts stating that so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date                                        text_concat\n",
       "0 2019-11-05 04:13:35     I’ve seen a couple of posts stating that so..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T21:22:35.590683Z",
     "start_time": "2019-11-07T21:22:35.588137Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Corpora\n",
    "corpora = df['text_concat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model, View Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T21:26:17.306717Z",
     "start_time": "2019-11-07T21:25:42.725737Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Vectorizer\n",
    "swds = custom_stop_words([swds_reddit_and_web, swds_artifacts])\n",
    "tfidf_vect = TfidfVectorizer(max_df=0.8, min_df=2, stop_words=swds)\n",
    "\n",
    "# Fit vectorizer to create document-term matrix\n",
    "doc_term_matrix = tfidf_vect.fit_transform(corpora.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T22:26:21.676511Z",
     "start_time": "2019-11-07T22:25:57.281815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "    n_components=10, random_state=42, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic Modeling\n",
    "nmf = NMF(n_components=10, random_state=42)\n",
    "\n",
    "nmf.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T21:27:12.972607Z",
     "start_time": "2019-11-07T21:26:58.588929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['one', 'romantic', 'partner', 'polyamorous', 'months', 'wants', 'opening', 'polyamory', 'needs', 'new', 'end', 'work', 'together', 'monogamous', 'long', 'years', 'want', 'relationships', 'open', 'relationship']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['others', 'mean', 'everyone', 'someone', 'way', 'non', 'many', 'multiple', 'different', 'polyamorous', 'partners', 'monogamous', 'person', 'like', 'monogamy', 'think', 'one', 'relationships', 'polyamory', 'people']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['great', 'one', 'first', 'bf', 'good', 'told', 'gf', 'cute', 'night', 'friends', 'together', 'went', 'friend', 'got', 'like', 'really', 'us', 'girlfriend', 'happy', 'boyfriend']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['condom', 'use', 'physical', 'romantic', 'intimate', 'asexual', 'oral', 'libido', 'casual', 'desire', 'get', 'drive', 'tested', 'intimacy', 'sexually', 'risk', 'partners', 'condoms', 'sexual', 'sex']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['nre', 'feel', 'like', 'much', 'make', 'days', 'home', 'needs', 'night', 'day', 'things', 'alone', 'get', 'one', 'work', 'spend', 'need', 'week', 'together', 'time']\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "['closed', 'privilege', 'together', 'want', 'woman', 'three', 'bi', 'one', 'separately', 'triads', 'us', 'couples', 'looking', 'hunters', 'third', 'hunting', 'unicorns', 'couple', 'triad', 'unicorn']\n",
      "\n",
      "\n",
      "Top 10 words for topic #6:\n",
      "['nre', 'always', 'never', 'still', 'fall', 'feelings', 'loves', 'one', 'beautiful', 'thank', 'heart', 'much', 'person', 'loved', 'loving', 'feel', 'someone', 'life', 'happy', 'love']\n",
      "\n",
      "\n",
      "Top 10 words for topic #7:\n",
      "['marriage', 'years', 'father', 'daughter', 'legal', 'married', 'live', 'know', 'life', 'kid', 'mother', 'would', 'parent', 'dad', 'mom', 'child', 'children', 'parents', 'family', 'kids']\n",
      "\n",
      "\n",
      "Top 10 words for topic #8:\n",
      "['podcast', 'thank', 'guide', 'ethical', 'new', 'youtu', 'f1f67abbbd49', 'polyamoryschool', 'resources', 'opening', 'skipped', 'multiamory', 'read', 'comments', 'unicorns', 'coupleprivilege', 'html', 'polyamory', 'morethantwo', 'https']\n",
      "\n",
      "\n",
      "Top 10 words for topic #9:\n",
      "['polyamory', 'partner', 'single', 'cheat', 'years', 'open', 'polyamorous', 'relationships', 'know', 'never', 'monogamy', 'community', 'work', 'happy', 'cheating', 'lifestyle', 'time', 'monogamous', 'mono', 'poly']\n",
      "\n",
      "\n",
      "Top 10 words for topic #10:\n",
      "['situation', 'another', 'going', 'counseling', 'op', 'man', 'swinging', 'spouse', 'years', 'wants', 'open', 'couple', 'us', 'woman', 'divorce', 'girlfriend', 'gf', 'married', 'marriage', 'wife']\n",
      "\n",
      "\n",
      "Top 10 words for topic #11:\n",
      "['another', 'want', 'meeting', 'also', 'boundary', 'shared', 'relationship', 'meet', 'would', 'one', 'np', 'boundaries', 'metamour', 'person', 'nesting', 'new', 'metas', 'partners', 'meta', 'partner']\n",
      "\n",
      "\n",
      "Top 10 words for topic #12:\n",
      "['girl', 'gay', 'girls', 'think', 'like', 'one', 'guy', 'opp', 'bisexual', 'guys', 'penis', 'gender', 'female', 'straight', 'male', 'bi', 'man', 'woman', 'men', 'women']\n",
      "\n",
      "\n",
      "Top 10 words for topic #13:\n",
      "['else', 'issues', 'compersion', 'deal', 'self', 'insecurities', 'things', 'good', 'get', 'really', 'help', 'insecurity', 'emotions', 'work', 'fear', 'feeling', 'feel', 'feelings', 'jealous', 'jealousy']\n",
      "\n",
      "\n",
      "Top 10 words for topic #14:\n",
      "['local', 'group', 'good', 'open', 'online', 'area', 'meetup', 'community', 'groups', 'tinder', 'poly', 'dating', 'friends', 'meet', 'okcupid', 'find', 'okc', 'looking', 'profile', 'people']\n",
      "\n",
      "\n",
      "Top 10 words for topic #15:\n",
      "['well', 'time', 'first', 'new', 'woman', 'feelings', 'husbands', 'open', 'spouse', 'would', 'divorce', 'bf', 'us', 'friend', 'years', 'hubby', 'boyfriend', 'married', 'marriage', 'husband']\n",
      "\n",
      "\n",
      "Top 10 words for topic #16:\n",
      "['couple', 'went', 'tell', 'meet', 'know', 'ask', 'going', 'would', 'separately', 'go', 'new', 'interested', 'time', 'people', 'person', 'someone', 'first', 'dates', 'dating', 'date']\n",
      "\n",
      "\n",
      "Top 10 words for topic #17:\n",
      "['nesting', 'rules', 'labels', 'someone', 'different', 'partner', 'prescriptive', 'important', 'descriptive', 'equal', 'married', 'secondaries', 'partners', 'primaries', 'relationships', 'relationship', 'hierarchical', 'hierarchy', 'secondary', 'primary']\n",
      "\n",
      "\n",
      "Top 10 words for topic #18:\n",
      "['sounds', 'tell', 'make', 'maybe', 'might', 'say', 'way', 'ask', 'things', 'feelings', 'something', 'need', 'really', 'talk', 'think', 'would', 'know', 'feel', 'like', 'want']\n",
      "\n",
      "\n",
      "Top 10 words for topic #19:\n",
      "['situation', 'lying', 'boundary', 'told', 'make', 'get', 'leave', 'tell', 'need', 'said', 'behavior', 'rule', 'going', 'someone', 'would', 'relationship', 'rules', 'trust', 'boundaries', 'cheating']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(nmf.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-20:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T21:41:07.574328Z",
     "start_time": "2019-11-07T21:41:07.526274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37788"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T21:41:11.381828Z",
     "start_time": "2019-11-07T21:41:11.377578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37788"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nmf.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T22:26:34.077636Z",
     "start_time": "2019-11-07T22:26:26.204309Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame()\n",
    "topic_dict = {}\n",
    "\n",
    "topics_list = []\n",
    "\n",
    "for i,component in enumerate(nmf.components_):\n",
    "#     print(f'Top 10 words for topic #{i}:')\n",
    "    top_words = [(tfidf_vect.get_feature_names()[i], round(component[i], 3)) for i in reversed(component.argsort()[-20:])]\n",
    "#     print(top_words)\n",
    "#     print('\\n')\n",
    "    topic_dict[i] = top_words\n",
    "#     for (word, importance) in top_words:\n",
    "#         topic_dict[i] += [(word, importance)]\n",
    "\n",
    "    topics_list += [[i, word, importance] for (word, importance) in top_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T22:26:43.151747Z",
     "start_time": "2019-11-07T22:26:43.147598Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_output = pd.DataFrame.from_records(topics_list, columns=['topic', 'word', 'component'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T22:26:47.037262Z",
     "start_time": "2019-11-07T22:26:47.031936Z"
    }
   },
   "outputs": [],
   "source": [
    "suffix = 'r_relationships_2012and2019_10_topics.csv'\n",
    "\n",
    "with open('../data/for_visualization/' + suffix, 'w') as file:\n",
    "    file.write(topic_output.to_csv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T01:26:47.241324Z",
     "start_time": "2019-11-07T01:26:05.053006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "    n_components=15, random_state=42, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic Modeling\n",
    "nmf = NMF(n_components=15, random_state=42)\n",
    "\n",
    "nmf.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T01:27:02.924368Z",
     "start_time": "2019-11-07T01:26:51.635865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['us', 'romantic', 'new', 'would', 'term', 'end', 'person', 'work', 'needs', 'secondary', 'one', 'years', 'monogamous', 'long', 'together', 'primary', 'open', 'want', 'relationships', 'relationship']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['lot', 'multiple', 'monogamous', 'everyone', 'non', 'way', 'polyamorous', 'someone', 'many', 'friends', 'different', 'partners', 'person', 'monogamy', 'polyamory', 'like', 'think', 'relationships', 'one', 'people']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['great', 'well', 'told', 'night', 'know', 'gf', 'years', 'first', 'good', 'went', 'together', 'got', 'girlfriend', 'friend', 'like', 'really', 'us', 'friends', 'happy', 'boyfriend']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['casual', 'men', 'unprotected', 'would', 'libido', 'condom', 'women', 'oral', 'desire', 'want', 'get', 'drive', 'tested', 'intimacy', 'sexually', 'risk', 'partners', 'condoms', 'sexual', 'sex']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['needs', 'dates', 'make', 'days', 'feel', 'home', 'things', 'day', 'alone', 'one', 'work', 'like', 'night', 'get', 'spend', 'need', 'date', 'together', 'week', 'time']\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "['couples', 'would', 'like', 'man', 'okc', 'interested', 'triad', 'want', 'bi', 'people', 'couple', 'find', 'profile', 'woman', 'looking', 'men', 'unicorn', 'date', 'dating', 'women']\n",
      "\n",
      "\n",
      "Top 10 words for topic #6:\n",
      "['feelings', 'way', 'never', 'fall', 'time', 'want', 'loves', 'thank', 'beautiful', 'much', 'heart', 'loved', 'person', 'someone', 'loving', 'one', 'feel', 'life', 'happy', 'love']\n",
      "\n",
      "\n",
      "Top 10 words for topic #7:\n",
      "['years', 'father', 'daughter', 'marriage', 'legal', 'live', 'know', 'married', 'life', 'kid', 'mother', 'would', 'dad', 'parent', 'mom', 'child', 'children', 'parents', 'family', 'kids']\n",
      "\n",
      "\n",
      "Top 10 words for topic #8:\n",
      "['thank', 'f1f67abbbd49', 'polyamoryschool', 'ethical', 'rules', 'resources', 'new', 'opening', 'skipped', 'unicorn', 'multiamory', 'us', 'comments', 'read', 'coupleprivilege', 'unicorns', 'html', 'polyamory', 'morethantwo', 'https']\n",
      "\n",
      "\n",
      "Top 10 words for topic #9:\n",
      "['partner', 'happy', 'years', 'relationships', 'work', 'time', 'know', 'find', 'polyamorous', 'lifestyle', 'community', 'dating', 'polyamory', 'monogamy', 'cheating', 'people', 'open', 'monogamous', 'mono', 'poly']\n",
      "\n",
      "\n",
      "Top 10 words for topic #10:\n",
      "['work', 'counseling', 'home', 'swinging', 'wants', 'op', 'open', 'years', 'spouse', 'man', 'couple', 'cheating', 'us', 'woman', 'divorce', 'gf', 'girlfriend', 'married', 'marriage', 'wife']\n",
      "\n",
      "\n",
      "Top 10 words for topic #11:\n",
      "['boundary', 'hierarchy', 'meet', 'date', 'np', 'one', 'secondary', 'dating', 'metamour', 'someone', 'would', 'boundaries', 'metas', 'new', 'person', 'nesting', 'primary', 'partners', 'meta', 'partner']\n",
      "\n",
      "\n",
      "Top 10 words for topic #12:\n",
      "['everyone', 'get', 'happy', 'feelings', 'open', 'time', 'would', 'husbands', 'spouse', 'woman', 'friend', 'divorce', 'bf', 'us', 'years', 'hubby', 'boyfriend', 'married', 'marriage', 'husband']\n",
      "\n",
      "\n",
      "Top 10 words for topic #13:\n",
      "['lot', 'something', 'way', 'insecurity', 'think', 'partner', 'get', 'good', 'emotions', 'help', 'work', 'things', 'fear', 'really', 'like', 'feeling', 'jealous', 'feelings', 'feel', 'jealousy']\n",
      "\n",
      "\n",
      "Top 10 words for topic #14:\n",
      "['boundaries', 'way', 'things', 'something', 'make', 'really', 'talk', 'said', 'say', 'someone', 'going', 'relationship', 'tell', 'feel', 'need', 'think', 'know', 'would', 'like', 'want']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(nmf.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-20:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T22:25:43.801827Z",
     "start_time": "2019-11-07T22:25:35.799549Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-475805f61612>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNMF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/sklearn/decomposition/nmf.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \"\"\"\n\u001b[0;32m-> 1297\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/sklearn/decomposition/nmf.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, W, H)\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m             \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m             shuffle=self.shuffle)\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         self.reconstruction_err_ = _beta_divergence(X, W, H, self.beta_loss,\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/sklearn/decomposition/nmf.py\u001b[0m in \u001b[0;36mnon_negative_factorization\u001b[0;34m(X, W, H, n_components, init, update_H, solver, beta_loss, tol, max_iter, alpha, l1_ratio, regularization, random_state, verbose, shuffle)\u001b[0m\n\u001b[1;32m   1055\u001b[0m                                                \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m                                                \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m                                                random_state=random_state)\n\u001b[0m\u001b[1;32m   1058\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         W, H, n_iter = _fit_multiplicative_update(X, W, H, beta_loss, max_iter,\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/sklearn/decomposition/nmf.py\u001b[0m in \u001b[0;36m_fit_coordinate_descent\u001b[0;34m(X, W, H, tol, max_iter, l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H, update_H, verbose, shuffle, random_state)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;31m# Update W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         violation += _update_coordinate_descent(X, W, Ht, l1_reg_W,\n\u001b[0;32m--> 505\u001b[0;31m                                                 l2_reg_W, shuffle, rng)\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;31m# Update H\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mupdate_H\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/sklearn/decomposition/nmf.py\u001b[0m in \u001b[0;36m_update_coordinate_descent\u001b[0;34m(X, W, Ht, l1_reg, l2_reg, shuffle, random_state)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0mHHt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m     \u001b[0mXHt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;31m# L2 regularization corresponds to increase of the diagonal of HHt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \"\"\"\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"toarray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_multivector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sparsetools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_matvecs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         fn(M, N, n_vecs, self.indptr, self.indices, self.data,\n\u001b[0;32m--> 487\u001b[0;31m            other.ravel(), result.ravel())\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Topic Modeling\n",
    "nmf = NMF(n_components=10, random_state=42)\n",
    "\n",
    "nmf.fit(doc_term_matrix)\n",
    "\n",
    "for i,topic in enumerate(nmf.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-20:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T01:28:08.599106Z",
     "start_time": "2019-11-07T01:27:49.426808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['get', 'time', 'talk', 'make', 'something', 'way', 'work', 'going', 'someone', 'things', 'really', 'think', 'know', 'need', 'would', 'feelings', 'feel', 'like', 'want', 'relationship']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['https', 'like', 'non', 'find', 'someone', 'polyamorous', 'think', 'open', 'love', 'person', 'dating', 'monogamy', 'mono', 'one', 'monogamous', 'polyamory', 'relationships', 'relationship', 'people', 'poly']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['good', 'first', 'night', 'friends', 'really', 'girlfriend', 'get', 'kids', 'years', 'like', 'one', 'family', 'happy', 'boyfriend', 'together', 'us', 'love', 'time', 'husband', 'wife']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['man', 'wife', 'sexually', 'risk', 'also', 'someone', 'partners', 'condoms', 'one', 'woman', 'get', 'think', 'want', 'would', 'like', 'people', 'men', 'women', 'sexual', 'sex']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['nesting', 'someone', 'want', 'feel', 'like', 'need', 'needs', 'person', 'secondary', 'relationships', 'together', 'date', 'one', 'new', 'meta', 'primary', 'relationship', 'partners', 'time', 'partner']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Topic Modeling\n",
    "nmf = NMF(n_components=5, random_state=42)\n",
    "\n",
    "nmf.fit(doc_term_matrix)\n",
    "\n",
    "for i,topic in enumerate(nmf.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-20:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T01:29:34.616598Z",
     "start_time": "2019-11-07T01:28:13.228292Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['say', 'tell', 'try', 'change', 'feelings', 'give', 'think', 'go', 'someone', 'may', 'might', 'needs', 'ask', 'talk', 'make', 'know', 'would', 'wants', 'need', 'want']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['romantic', 'everyone', 'way', 'others', 'life', 'someone', 'many', 'non', 'different', 'multiple', 'partners', 'polyamorous', 'monogamous', 'person', 'think', 'one', 'monogamy', 'relationships', 'polyamory', 'people']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['sharing', 'wonderful', 'nice', 'glad', 'lovely', 'sweet', 'good', 'hope', 'guys', 'thanks', 'amazing', 'congrats', 'great', 'lol', 'awesome', 'adorable', 'beautiful', 'thank', 'cute', 'happy']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['partner', 'partners', 'swinging', 'emotional', 'attracted', 'ace', 'enjoy', 'connection', 'physical', 'intimate', 'romantic', 'asexual', 'casual', 'libido', 'desire', 'drive', 'sexually', 'intimacy', 'sexual', 'sex']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['new', 'home', 'long', 'every', 'spending', 'see', 'work', 'alone', 'plans', 'schedule', 'partners', 'night', 'days', 'need', 'one', 'day', 'spend', 'together', 'week', 'time']\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "['closed', 'bisexual', 'find', 'privilege', 'woman', 'three', 'bi', 'one', 'separately', 'triads', 'us', 'couples', 'looking', 'third', 'hunters', 'hunting', 'unicorns', 'couple', 'triad', 'unicorn']\n",
      "\n",
      "\n",
      "Top 10 words for topic #6:\n",
      "['different', 'falling', 'way', 'still', 'much', 'feelings', 'always', 'like', 'never', 'heart', 'fall', 'life', 'loves', 'feel', 'person', 'loved', 'loving', 'one', 'someone', 'love']\n",
      "\n",
      "\n",
      "Top 10 words for topic #7:\n",
      "['baby', 'marriage', 'live', 'know', 'father', 'daughter', 'legal', 'married', 'life', 'would', 'kid', 'mother', 'parent', 'dad', 'mom', 'child', 'children', 'parents', 'family', 'kids']\n",
      "\n",
      "\n",
      "Top 10 words for topic #8:\n",
      "['book', 'podcast', 'guide', 'ethical', 'new', 'youtu', 'f1f67abbbd49', 'polyamoryschool', 'resources', 'opening', 'skipped', 'multiamory', 'read', 'comments', 'unicorns', 'coupleprivilege', 'html', 'polyamory', 'morethantwo', 'https']\n",
      "\n",
      "\n",
      "Top 10 words for topic #9:\n",
      "['still', 'folks', 'really', 'happy', 'polyamorous', 'open', 'single', 'partner', 'monogamy', 'relationships', 'know', 'years', 'time', 'work', 'never', 'community', 'lifestyle', 'monogamous', 'mono', 'poly']\n",
      "\n",
      "\n",
      "Top 10 words for topic #10:\n",
      "['well', 'another', 'counseling', 'open', 'going', 'op', 'man', 'swinging', 'spouse', 'wants', 'years', 'couple', 'us', 'woman', 'divorce', 'gf', 'girlfriend', 'married', 'marriage', 'wife']\n",
      "\n",
      "\n",
      "Top 10 words for topic #11:\n",
      "['meeting', 'shared', 'also', 'like', 'boundary', 'would', 'feel', 'one', 'meet', 'np', 'boundaries', 'relationship', 'metamour', 'new', 'nesting', 'person', 'metas', 'partners', 'meta', 'partner']\n",
      "\n",
      "\n",
      "Top 10 words for topic #12:\n",
      "['lesbian', 'another', 'think', 'gay', 'girls', 'guy', 'one', 'guys', 'opp', 'bisexual', 'penis', 'gender', 'female', 'straight', 'male', 'bi', 'man', 'woman', 'men', 'women']\n",
      "\n",
      "\n",
      "Top 10 words for topic #13:\n",
      "['felt', 'years', 'wanted', 'started', 'still', 'close', 'best', 'really', 'bf', 'gf', 'us', 'feelings', 'ex', 'know', 'girlfriend', 'friendship', 'told', 'boyfriend', 'friends', 'friend']\n",
      "\n",
      "\n",
      "Top 10 words for topic #14:\n",
      "['group', 'local', 'friends', 'fetlife', 'online', 'area', 'open', 'meetup', 'community', 'groups', 'poly', 'tinder', 'dating', 'meet', 'find', 'okcupid', 'okc', 'looking', 'profile', 'people']\n",
      "\n",
      "\n",
      "Top 10 words for topic #15:\n",
      "['think', 'new', 'open', 'well', 'get', 'girlfriend', 'first', 'woman', 'husbands', 'would', 'spouse', 'divorce', 'bf', 'us', 'hubby', 'years', 'married', 'boyfriend', 'marriage', 'husband']\n",
      "\n",
      "\n",
      "Top 10 words for topic #16:\n",
      "['get', 'start', 'ask', 'meeting', 'couple', 'went', 'meet', 'know', 'separately', 'interested', 'going', 'go', 'new', 'people', 'person', 'someone', 'first', 'dates', 'dating', 'date']\n",
      "\n",
      "\n",
      "Top 10 words for topic #17:\n",
      "['term', 'non', 'labels', 'different', 'nesting', 'partner', 'prescriptive', 'important', 'equal', 'descriptive', 'married', 'secondaries', 'partners', 'primaries', 'relationships', 'relationship', 'hierarchical', 'hierarchy', 'secondary', 'primary']\n",
      "\n",
      "\n",
      "Top 10 words for topic #18:\n",
      "['guy', 'even', 'ask', 'could', 'seems', 'things', 'get', 'maybe', 'thing', 'said', 'way', 'say', 'something', 'really', 'sounds', 'know', 'would', 'think', 'feel', 'like']\n",
      "\n",
      "\n",
      "Top 10 words for topic #19:\n",
      "['normal', 'something', 'insecure', 'emotion', 'envy', 'time', 'insecurities', 'deal', 'else', 'get', 'compersion', 'work', 'emotions', 'insecurity', 'feeling', 'fear', 'feel', 'feelings', 'jealous', 'jealousy']\n",
      "\n",
      "\n",
      "Top 10 words for topic #20:\n",
      "['back', 'take', 'still', 'life', 'feeling', 'much', 'need', 'self', 'get', 'better', 'going', 'good', 'therapy', 'work', 'feelings', 'hard', 'really', 'help', 'things', 'feel']\n",
      "\n",
      "\n",
      "Top 10 words for topic #21:\n",
      "['said', 'agree', 'need', 'agreement', 'respect', 'agreed', 'behavior', 'consent', 'someone', 'lying', 'tell', 'cheated', 'boundary', 'would', 'relationship', 'rule', 'trust', 'boundaries', 'rules', 'cheating']\n",
      "\n",
      "\n",
      "Top 10 words for topic #22:\n",
      "['dating', 'another', 'polyamory', 'term', 'polyamorous', 'two', 'opening', 'work', 'one', 'would', 'new', 'end', 'years', 'long', 'together', 'monogamous', 'like', 'relationships', 'open', 'relationship']\n",
      "\n",
      "\n",
      "Top 10 words for topic #23:\n",
      "['hiv', 'protection', 'fluid', 'get', 'std', 'health', 'unprotected', 'hpv', 'testing', 'condom', 'sti', 'oral', 'use', 'partners', 'herpes', 'test', 'hsv', 'tested', 'risk', 'condoms']\n",
      "\n",
      "\n",
      "Top 10 words for topic #24:\n",
      "['alone', 'sheets', 'three', 'two', 'living', 'get', 'middle', 'us', 'bedroom', 'together', 'one', 'space', 'sleeping', 'king', 'home', 'house', 'night', 'room', 'sleep', 'bed']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Topic Modeling\n",
    "nmf = NMF(n_components=25, random_state=42)\n",
    "\n",
    "nmf.fit(doc_term_matrix)\n",
    "\n",
    "for i,topic in enumerate(nmf.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-20:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Strength Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Reconstruction Error\n",
    "recon_errors = []\n",
    "\n",
    "for n_components in range(2, 40):\n",
    "\n",
    "    nmf_model = NMF(n_components=n_components)\n",
    "    doc_topic = nmf_model.fit_transform(coo.toarray())\n",
    "    recon_errors += [[n_components, nmf_model.reconstruction_err_]]\n",
    "\n",
    "df = pd.DataFrame(recon_errors, columns = ['N', 'Reconstruction Error'])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
